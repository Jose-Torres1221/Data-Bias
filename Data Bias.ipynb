{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c4875ac",
   "metadata": {},
   "source": [
    "# Initializing and Reading CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc0a45ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "labeledComments = pd.read_csv(\"Sample_labaled_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052a20b",
   "metadata": {},
   "source": [
    "# Finding Toxicity and Storing in a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d504c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity = list(labeledComments[\"toxic\"])\n",
    "\n",
    "comments = list(labeledComments[\"comment_text\"])\n",
    "\n",
    "toxic_comments = []\n",
    "not_toxic_comments = []\n",
    "\n",
    "for i in range(len(comments)):\n",
    "    toxic = toxicity[i]\n",
    "    comment = comments[i]\n",
    "    \n",
    "    if toxic == 'yes':\n",
    "        toxic_comments.append(comment)\n",
    "    else:\n",
    "        not_toxic_comments.append(comment)\n",
    "        \n",
    "#Limiting Amount of Elements because of Quota Limit\n",
    "toxic_comments = toxic_comments[645:675]\n",
    "not_toxic_comments = not_toxic_comments[645:675]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea2b9a0",
   "metadata": {},
   "source": [
    "# Averaging Toxicity Score for Toxic and Non-Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44aa0b56",
   "metadata": {},
   "outputs": [],
   "source": [
    " from googleapiclient import discovery\n",
    "import json\n",
    "\n",
    "API_KEY = 'insert-api-key'\n",
    "\n",
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "\n",
    "#Function to find average toxicity\n",
    "def avgToxic(list_of_comments):\n",
    "    toxic_score = 0\n",
    "    \n",
    "    for comment in list_of_comments:\n",
    "        analyze_request = {\n",
    "              'comment': { 'text': comment },\n",
    "              'requestedAttributes': {'TOXICITY': {}}\n",
    "        }\n",
    "        \n",
    "        response = client.comments().analyze(body=analyze_request).execute()\n",
    "        final = response[\"attributeScores\"][\"TOXICITY\"][\"spanScores\"][0][\"score\"][\"value\"]\n",
    "        toxic_score += final\n",
    "        \n",
    "    avgToxicity = toxic_score/(len(list_of_comments))\n",
    "    \n",
    "    return avgToxicity\n",
    "\n",
    "#Getting AVG\n",
    "avgToxicity_y = avgToxic(toxic_comments)\n",
    "avgToxicity_n = avgToxic(not_toxic_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d0dfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probable threshold for toxicity is: 46.35%\n"
     ]
    }
   ],
   "source": [
    "#Creating a threshold\n",
    "prob_threshold = (avgToxicity_y + avgToxicity_n)/2\n",
    "\n",
    "prob_threshold = round(prob_threshold*100, 2)\n",
    "\n",
    "print(f'The probable threshold for toxicity is: {prob_threshold}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05f54e",
   "metadata": {},
   "source": [
    "# Forming A Hypothesis and Tests\n",
    "\n",
    "\n",
    "## Hypothesis\n",
    "From what I learned about the way the model works and how the toxicty scores are assigned, my hypothesis is that profanity is needlessly labeled as toxic. In my opinion works like \"fuck\" and \"shit\" can be used positively depending on context, but I believe the model cannot read the context and disproportionately labels comments with profanity toxic.\n",
    "\n",
    "\n",
    "## Tests\n",
    "To test my hypothesis I will create a CSV of 60 comments of which I will label as toxic or not myself. Then I will use my threshold to have the model apply a \"yes\" or \"no\" label to the comments then test for false positives, false negatives, true positives, and true negatives. Each comment will be less than 15 words to prevent a long vs short bias, and comments will use gender neutral terms as well to avoid a gender bias.\n",
    "\n",
    "## Low Sample Size\n",
    "I am using a limit of 60 comments because my quota is 60 requests per minute, this limits my ability to create a larger sample size and reduce the possibility of it being an exception instead of a part of the rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ccbbf3",
   "metadata": {},
   "source": [
    "# Testing my Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "036ba565",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = pd.read_csv(\"josecomments.csv\")\n",
    "test_comments.head()\n",
    "\n",
    "toxicity_J = list(test_comments[\"toxic\"])\n",
    "\n",
    "comments_J = list(test_comments[\"comment\"])\n",
    "\n",
    "comment_toxic_dictionary = {}\n",
    "toxic_dict_scores = {}\n",
    "\n",
    "for comment in comments_J:\n",
    "    analyze_request = {\n",
    "          'comment': { 'text': comment },\n",
    "          'requestedAttributes': {'TOXICITY': {}}\n",
    "    }\n",
    "\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    final = response[\"attributeScores\"][\"TOXICITY\"][\"spanScores\"][0][\"score\"][\"value\"]\n",
    "    \n",
    "    final = round(final*100, 2)\n",
    "   \n",
    "    toxic_dict_scores[comment] = final\n",
    "    \n",
    "    if final > prob_threshold:\n",
    "        comment_toxic_dictionary[comment] = 1\n",
    "    \n",
    "    elif final < prob_threshold:\n",
    "        comment_toxic_dictionary[comment] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a37ae48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for True Positives (yes/yes) is 0.82.\n",
      "The accuracy score for True Negatives (no/no) is 0.37.\n"
     ]
    }
   ],
   "source": [
    "predicted_toxicity = list(comment_toxic_dictionary.values())\n",
    "\n",
    "def class_wise_acc(y_actual, y_predicted):\n",
    "    total_p = 0\n",
    "    total_n = 0\n",
    "    TP=0\n",
    "    TN=0\n",
    "    for i in range(len(y_predicted)):\n",
    "        if y_actual[i]==1:\n",
    "            total_p = total_p+1\n",
    "            if y_actual[i]==y_predicted[i]:\n",
    "               TP=TP+1\n",
    "        if y_actual[i]==0:\n",
    "            total_n=total_n+1\n",
    "            if y_actual[i]==y_predicted[i]:\n",
    "               TN=TN+1\n",
    "    return(TP/total_p, TN/total_n)\n",
    "\n",
    "class_yes_acc_toxicity, class_no_acc_toxicity = class_wise_acc(toxicity_J, predicted_toxicity)\n",
    "\n",
    "print(f'The accuracy score for True Positives (yes/yes) is {class_yes_acc_toxicity:.2f}.')\n",
    "print(f'The accuracy score for True Negatives (no/no) is {class_no_acc_toxicity:.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa243c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('test_toxic_scores.csv', 'w') as f:\n",
    "    for key in toxic_dict_scores.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,toxic_dict_scores[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
